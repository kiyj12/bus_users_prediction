{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# data preprocessing\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 130, 'max_rows', 30)\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# ignore warining\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import folium # 지도 관련 시각화\n",
    "from folium.plugins import MarkerCluster #지도 관련 시각화\n",
    "import geopy.distance #거리 계산해주는 패키지 사용\n",
    "\n",
    "\n",
    "# save\n",
    "import joblib \n",
    "import pickle\n",
    "\n",
    "# selenium\n",
    "from selenium.webdriver import Chrome\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import joblib \n",
    "import pickle\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from keras import metrics\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 21), (228170, 20), (2409414, 13))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "bts = pd.read_csv(\"bus_bts.csv\")\n",
    "\n",
    "train.shape, test.shape, bts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_weather() :\n",
    "    \n",
    "    weather_data_10 = pd.DataFrame(columns=['현재일기_10','현재기온_10','체감온도_10','일강수_10'])\n",
    "    browser = Chrome()\n",
    "    url = 'http://www.weather.go.kr/weather/observation/currentweather.jsp?auto_man=m&type=t99&reg=184&tm=2019.10.25.16%3A00&x=19&y=7'\n",
    "    browser.get(url)\n",
    "\n",
    "    for i in range(0,46):\n",
    "        i+=1\n",
    "\n",
    "        elem=browser.find_element_by_id('observation_text')\n",
    "        elem.clear()\n",
    "        elem.send_keys(\"2019.9.{}.10:00\".format(i))\n",
    "\n",
    "        btn=browser.find_elements_by_class_name('btn')\n",
    "        btn[2].click()\n",
    "        \n",
    "        time.sleep(1)\n",
    "        weathers = browser.find_elements_by_css_selector('td')\n",
    "        weather_data_10 = weather_data_10.append(pd.DataFrame([[weathers[40].text,weathers[44].text, weathers[46].text, weathers[47].text]],columns=['현재일기_10','현재기온_10','체감온도_10','일강수_10']))\n",
    "        \n",
    "            \n",
    "    print('success !')\n",
    "    browser.close()\n",
    "    \n",
    "    return weather_data_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success !\n"
     ]
    }
   ],
   "source": [
    "weather_data = crawl_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save.. !\n"
     ]
    }
   ],
   "source": [
    "weather_data.to_csv('weather.csv', index = False)\n",
    "\n",
    "print('save.. !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 22), (228170, 21))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['weekday'] = train['date'].dt.weekday\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['weekday'] = test['date'].dt.weekday\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 23), (228170, 22))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['bus_route_id'] = train['bus_route_id'].astype(str)\n",
    "train['station_code'] = train['station_code'].astype(str)\n",
    "train['route_station'] = train['bus_route_id'] + ',' + train['station_code']\n",
    "\n",
    "test['bus_route_id'] = test['bus_route_id'].astype(str)\n",
    "test['station_code'] = test['station_code'].astype(str)\n",
    "test['route_station'] = test['bus_route_id'] + ',' + test['station_code']\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 24), (228170, 23))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['bus_route_id_weekday'] = train['bus_route_id'].astype(str) + ',' + train['weekday'].astype(str) \n",
    "test['bus_route_id_weekday'] = test['bus_route_id'].astype(str) + ',' + test['weekday'].astype(str) \n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 25), (228170, 24))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['station_code_weekday'] = train['station_code'].astype(str) + ',' + train['weekday'].astype(str)\n",
    "test['station_code_weekday'] = test['station_code'].astype(str) + ',' + test['weekday'].astype(str)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 26), (228170, 25))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['route_station_weekday'] = train['route_station'].astype(str) + ',' + train['weekday'].astype(str) \n",
    "test['route_station_weekday'] = test['route_station'].astype(str) + ',' + test['weekday'].astype(str)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2409414, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bts['on_time']  = bts['geton_time'].apply(lambda x : x[:2])\n",
    "\n",
    "bts.iloc[bts.query('on_time == \"06\"').index,13] = '6~7_ride'\n",
    "bts.iloc[bts.query('on_time == \"07\"').index,13] = '7~8_ride'\n",
    "bts.iloc[bts.query('on_time == \"08\"').index,13] = '8~9_ride'\n",
    "bts.iloc[bts.query('on_time == \"09\"').index,13] = '9~10_ride'\n",
    "bts.iloc[bts.query('on_time == \"10\"').index,13] = '10~11_ride'\n",
    "bts.iloc[bts.query('on_time == \"11\"').index,13] = '11~12_ride'\n",
    "\n",
    "bts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 32), (228170, 31))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['68a']=train['6~7_ride']+train['7~8_ride'] \n",
    "train['810a']=train['8~9_ride']+train['9~10_ride']\n",
    "train['1012a']=train['10~11_ride']+train['11~12_ride']\n",
    "\n",
    "train['68b']=train['6~7_takeoff']+train['7~8_takeoff'] \n",
    "train['810b']=train['8~9_takeoff']+train['9~10_takeoff']\n",
    "train['1012b']=train['10~11_takeoff']+train['11~12_takeoff']\n",
    "\n",
    "test['68a']=test['6~7_ride']+test['7~8_ride']\n",
    "test['810a']=test['8~9_ride']+test['9~10_ride']\n",
    "test['1012a']=test['10~11_ride']+test['11~12_ride']\n",
    "\n",
    "test['68b']=test['6~7_takeoff']+test['7~8_takeoff']\n",
    "test['810b']=test['8~9_takeoff']+test['9~10_takeoff']\n",
    "test['1012b']=test['10~11_takeoff']+test['11~12_takeoff']\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_statistic(ID, col1, col2) :\n",
    "    \n",
    "    # mean, sum\n",
    "    rs_mean = train.groupby([ID])['18~20_ride'].agg([(col1, 'mean')]).reset_index()\n",
    "    rs_sum = train.groupby([ID])['18~20_ride'].agg([(col2, 'sum')]).reset_index()\n",
    "    rs_mean_sum = pd.merge(rs_mean, rs_sum, on=ID)\n",
    "\n",
    "    # merge\n",
    "    tr = pd.merge(train, rs_mean_sum, how='left', on=ID)\n",
    "    te = pd.merge(test, rs_mean_sum, how='left', on=ID)\n",
    "\n",
    "    # na -&gt; mean\n",
    "    te[col1] = te[col1].fillna(rs_mean.mean())\n",
    "    te[col1] = te[col1].fillna(rs_sum.mean())\n",
    "    \n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 34), (228170, 33))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = id_statistic('route_station', '1820_rs_mean', '1820_rs_sum')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 36), (228170, 35))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = id_statistic('bus_route_id', '1820_r_mean', '1820_r_sum')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 38), (228170, 37))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = id_statistic('station_code', '1820_s_mean', '1820_s_sum')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 40), (228170, 39))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = id_statistic('weekday', '1820_w_mean', '1820_w_sum')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_statistics() :\n",
    "\n",
    "    f = train.groupby(['bus_route_id_weekday'])['18~20_ride'].agg([('mean_bus_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(train, f, how='left', on='bus_route_id_weekday')\n",
    "    te = pd.merge(test, f, how='left', on='bus_route_id_weekday').fillna(f['mean_bus_weekday_ride'].mean())\n",
    "    \n",
    "    f = train.groupby(['station_code_weekday'])['18~20_ride'].agg([('mean_station_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(tr, f, how='left', on='station_code_weekday')\n",
    "    te = pd.merge(te, f, how='left', on='station_code_weekday').fillna(f['mean_station_weekday_ride'].mean())\n",
    "    \n",
    "    f = train.groupby(['route_station_weekday'])['18~20_ride'].agg([('mean_route_station_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(tr, f, how='left', on='route_station_weekday')\n",
    "    te = pd.merge(te, f, how='left', on='route_station_weekday').fillna(f['mean_route_station_weekday_ride'].mean())\n",
    "    \n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 43), (228170, 42))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = mean_statistics()\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def congestion() :\n",
    "    df = train.groupby(['bus_route_id'])['18~20_ride'].agg([('passenger', 'sum')])\n",
    "    df = df.sort_values(by='passenger', ascending=False).reset_index()\n",
    "    \n",
    "    def f(x):\n",
    "        if x > 10000:\n",
    "            return 7\n",
    "\n",
    "        elif x > 5000:\n",
    "            return 6\n",
    "\n",
    "        elif x > 2000:\n",
    "            return 5\n",
    "\n",
    "        elif x > 700:\n",
    "            return 4\n",
    "\n",
    "        elif x > 200:\n",
    "            return 3\n",
    "\n",
    "        elif x > 50:\n",
    "            return 2\n",
    "\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    df['congestion']=df['passenger'].apply(f)\n",
    "    df = df[['bus_route_id','congestion']]\n",
    "    \n",
    "    tr = pd.merge(train, df, how='left', on='bus_route_id')\n",
    "    te = pd.merge(test, df, how='left', on='bus_route_id')\n",
    "    \n",
    "    # 결측치는 데이터 프레임 df의 'congestion'의 중간값인 '4'으로 대체\n",
    "    te = te.fillna(4)\n",
    "    \n",
    "    return tr, te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 44), (228170, 43))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = congestion()\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 45), (228170, 44))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'] = train['latitude'].astype(str) + ',' + train['longitude'].astype(str)\n",
    "test['location'] = test['latitude'].astype(str) + ',' + test['longitude'].astype(str)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((415423, 46), (228170, 45))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make cue column\n",
    "train['cue']=0\n",
    "test['cue']=1\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morning() :\n",
    "    \n",
    "    # merge\n",
    "    data = pd.concat([train, test])\n",
    "    \n",
    "    a = data.groupby(['route_station'])['1012a'].agg({'sum', 'mean'}).reset_index()\n",
    "    a.columns = ['route_station', '1012a_sum','1012a_mean']\n",
    "\n",
    "    b = data.groupby(['route_station'])['1012b'].agg({'sum', 'mean'}).reset_index()\n",
    "    b.columns = ['route_station', '1012b_sum','1012b_mean']\n",
    "    b = b[['1012b_sum','1012b_mean']]\n",
    "\n",
    "    c = data.groupby(['route_station'])['10~11_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    c.columns = ['route_station', '10~11_ride_sum','10~11_ride_mean']\n",
    "    c = c[['10~11_ride_sum','10~11_ride_mean']]\n",
    "\n",
    "    d = data.groupby(['route_station'])['10~11_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    d.columns = ['route_station', '10~11_takeoff_sum','10~11_takeoff_mean']\n",
    "    d = d[['10~11_takeoff_sum','10~11_takeoff_mean']]\n",
    "\n",
    "    e = data.groupby(['route_station'])['11~12_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    e.columns = ['route_station', '11~12_ride_sum','11~12_ride_mean']\n",
    "    e = e[['11~12_ride_sum','11~12_ride_mean']]\n",
    "\n",
    "    f = data.groupby(['route_station'])['11~12_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    f.columns = ['route_station', '11~12_takeoff_sum','11~12_takeoff_mean']\n",
    "    f = f[['11~12_takeoff_sum','11~12_takeoff_mean']]\n",
    "\n",
    "    g = data.groupby(['route_station'])['1820_r_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    g.columns = ['route_station', '1820_r_mean_sum','1820_r_mean_mean']\n",
    "    g = g[['1820_r_mean_sum','1820_r_mean_mean']]\n",
    "\n",
    "    h = data.groupby(['route_station'])['1820_r_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    h.columns = ['route_station', '1820_r_sum_sum','1820_r_sum_mean']\n",
    "    h = h[['1820_r_sum_sum','1820_r_sum_mean']]\n",
    "\n",
    "    i = data.groupby(['route_station'])['1820_rs_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    i.columns = ['route_station', '1820_rs_mean_sum','1820_rs_mean_mean']\n",
    "    i = i[['1820_rs_mean_sum','1820_rs_mean_mean']]\n",
    "\n",
    "    j = data.groupby(['route_station'])['1820_rs_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    j.columns = ['route_station', '1820_rs_sum_sum','1820_rs_sum_mean']\n",
    "    j = j[['1820_rs_sum_sum','1820_rs_sum_mean']]\n",
    "\n",
    "    k = data.groupby(['route_station'])['1820_s_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    k.columns = ['route_station', '1820_s_mean_sum','1820_s_mean_mean']\n",
    "    k = k[['1820_s_mean_sum','1820_s_mean_mean']]\n",
    "\n",
    "    l = data.groupby(['route_station'])['1820_s_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    l.columns = ['route_station', '1820_s_sum_sum','1820_s_sum_mean']\n",
    "    l = l[['1820_s_sum_sum','1820_s_sum_mean']]\n",
    "\n",
    "    m = data.groupby(['route_station'])['1820_w_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    m.columns = ['route_station', '1820_w_mean_sum','1820_w_mean_mean']\n",
    "    m = m[['1820_w_mean_sum','1820_w_mean_mean']]\n",
    "\n",
    "    n = data.groupby(['route_station'])['1820_w_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    n.columns = ['route_station', '1820_w_sum_sum','1820_w_sum_mean']\n",
    "    n = n[['1820_w_sum_sum','1820_w_sum_mean']]\n",
    "\n",
    "    o = data.groupby(['route_station'])['68a'].agg({'sum', 'mean'}).reset_index()\n",
    "    o.columns = ['route_station', '68a_sum','68a_mean']\n",
    "    o = o[['68a_sum','68a_mean']]\n",
    "\n",
    "    p = data.groupby(['route_station'])['68b'].agg({'sum', 'mean'}).reset_index()\n",
    "    p.columns = ['route_station', '68b_sum','68b_mean']\n",
    "    p = p[['68b_sum','68b_mean']]\n",
    "\n",
    "    q = data.groupby(['route_station'])['6~7_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    q.columns = ['route_station', '6~7_ride_sum','6~7_ride_mean']\n",
    "    q = q[['6~7_ride_sum','6~7_ride_mean']]\n",
    "\n",
    "    r = data.groupby(['route_station'])['6~7_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    r.columns = ['route_station', '6~7_takeoff_sum','6~7_takeoff_mean']\n",
    "    r = r[['6~7_takeoff_sum','6~7_takeoff_mean']]\n",
    "\n",
    "    s = data.groupby(['route_station'])['7~8_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    s.columns = ['route_station', '7~8_ride_sum','7~8_ride_mean']\n",
    "    s = s[['7~8_ride_sum','7~8_ride_mean']]\n",
    "\n",
    "    t = data.groupby(['route_station'])['7~8_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    t.columns = ['route_station', '7~8_takeoff_sum','7~8_takeoff_mean']\n",
    "    t = t[['7~8_takeoff_sum','7~8_takeoff_mean']]\n",
    "\n",
    "    u = data.groupby(['route_station'])['810a'].agg({'sum', 'mean'}).reset_index()\n",
    "    u.columns = ['route_station', '810a_sum','810a_mean']\n",
    "    u = u[['810a_sum','810a_mean']]\n",
    "\n",
    "    v = data.groupby(['route_station'])['810b'].agg({'sum', 'mean'}).reset_index()\n",
    "    v.columns = ['route_station', '810b_sum','810b_mean']\n",
    "    v = v[['810b_sum','810b_mean']]\n",
    "\n",
    "    w = data.groupby(['route_station'])['8~9_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    w.columns = ['route_station', '8~9_ride_sum','8~9_ride_mean']\n",
    "    w = w[['8~9_ride_sum','8~9_ride_mean']]\n",
    "\n",
    "    x = data.groupby(['route_station'])['8~9_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    x.columns = ['route_station', '8~9_takeoff_sum','8~9_takeoff_mean']\n",
    "    x = x[['8~9_takeoff_sum','8~9_takeoff_mean']]\n",
    "\n",
    "    y = data.groupby(['route_station'])['9~10_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    y.columns = ['route_station', '9~10_ride_sum','9~10_ride_mean']\n",
    "    y = y[['9~10_ride_sum','9~10_ride_mean']]\n",
    "\n",
    "    z = data.groupby(['route_station'])['9~10_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    z.columns = ['route_station', '9~10_takeoff_sum','9~10_takeoff_mean']\n",
    "    z = z[['9~10_takeoff_sum','9~10_takeoff_mean']]\n",
    "    \n",
    "    df = pd.concat([a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z],axis=1)\n",
    "    df = pd.merge(data, df, how='left', on='route_station')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 98)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = morning()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bus_route_id'] = train['bus_route_id'].astype(np.int64)\n",
    "test['bus_route_id'] = test['bus_route_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts['geton_time2'] = pd.to_datetime(bts['geton_time'])\n",
    "\n",
    "f = bts.groupby(['geton_date','geton_time2','geton_station_code','bus_route_id'])['user_count'].\\\n",
    "agg([('탑승객_수','sum')]).reset_index().\\\n",
    "sort_values(by=['geton_date','geton_station_code','bus_route_id','geton_time2'], ascending=True).reset_index()\n",
    "\n",
    "f['index'] = list(range(0,len(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = []\n",
    "\n",
    "for i in range(0,len(f)-1):\n",
    "\n",
    "    if ((f.iloc[i].geton_date == f.iloc[i+1].geton_date) &\\\n",
    "        (f.iloc[i].geton_station_code == f.iloc[i+1].geton_station_code) &\\\n",
    "        (f.iloc[i].bus_route_id == f.iloc[i+1].bus_route_id)):\n",
    "\n",
    "        time.append(f.iloc[i+1].geton_time2 - f.iloc[i].geton_time2)\n",
    "\n",
    "    else:\n",
    "        time.append(0)\n",
    "\n",
    "time.insert(0, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sec(time_str):\n",
    "\n",
    "    h, m, s = time_str.split(':')\n",
    "\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bus_interval() :\n",
    "\n",
    "    f['time'] = time\n",
    "    f['time2'] = f['time'].astype(str).str[7:]\n",
    "\n",
    "\n",
    "    interval = f.copy()\n",
    "    interval['time2'] = interval['time2'].astype(str).replace('','00:00:00')\n",
    "    interval['bus_route_id'] = interval['bus_route_id'].astype(object)\n",
    "\n",
    "    time4 = []\n",
    "\n",
    "    for i in interval['time2'] :\n",
    "\n",
    "        time4.append(get_sec(i))\n",
    "\n",
    "    interval['time4'] = time4\n",
    "    interval['time4'] = (interval['time4'] / 60).astype(int)\n",
    "\n",
    "    interval = interval[interval['time4'] > 3] # 간격이 3분보다 작은 것 제외 \n",
    "    interval = interval[interval['time4'] < 180] # 간격이 3시간보다 큰 것 제외\n",
    "\n",
    "    interval = interval.groupby('bus_route_id')['time4'].agg([('bus_interval', 'mean')]).reset_index()\n",
    "    interval['bus_interval'] = interval['bus_interval'].astype(int)\n",
    "\n",
    "    # 나중에 시간을 절약하기 위해 csv 파일로 저장\n",
    "    interval.to_csv('bus_interval_final.csv', index = False)\n",
    "\n",
    "    print('success.. !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success.. !\n"
     ]
    }
   ],
   "source": [
    "bus_interval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_interval = pd.read_csv(\"bus_interval_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bus_route_id'] = data['bus_route_id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, bus_interval, how = 'left', on = 'bus_route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bus_interval'] = data['bus_interval'].fillna(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 103)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df_encode = data[['bus_route_id','station_code', 'route_station_weekday', 'route_station']]\n",
    "df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "\n",
    "data['bus_route_id2']=df_encoded['bus_route_id']\n",
    "data['station_code2']=df_encoded['station_code']\n",
    "data['route_station_weekday2']=df_encoded['route_station_weekday']\n",
    "data['route_station2']=df_encoded['route_station']\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather() :\n",
    "\n",
    "    weather_data = pd.read_csv('weather.csv', encoding = 'utf-8')\n",
    "    weather_data['id'] = range(0,46)\n",
    "\n",
    "    a = pd.DataFrame(data.date.unique(), columns=['date']) ; a['id'] = range(0,46)\n",
    "    weather_data = pd.merge(a, weather_data)\n",
    "    weather_data = weather_data[['date','현재일기_10','체감온도_10','일강수_10']]\n",
    "    weather_data = weather_data.replace(' ', 0)\n",
    "    df = pd.merge(data, weather_data, on='date')\n",
    "    # label_encoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    df_encode = df[['현재일기_10']]\n",
    "    df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "    df['현재일기_10']=df_encoded['현재일기_10']\n",
    "    # object-&gt;float 변수변환\n",
    "    df['현재일기_10'] = df['현재일기_10'].astype(float)\n",
    "    df['체감온도_10'] = df['체감온도_10'].astype(float)\n",
    "    df['일강수_10'] = df['일강수_10'].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 106)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = weather()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 113)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data = pd.get_dummies(data,columns=['weekday'])\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 113)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['in_out'].value_counts()\n",
    "data['in_out'] = data['in_out'].map({'시내':0,'시외':1})\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 124)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_jejusi = (33.500770, 126.522761) #제주시의 위도 경도\n",
    "data['dis_jejusi'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusi).km for i in range(len(data))]\n",
    "\n",
    "coords_jejusicheong1 = (33.49892, 126.53035) #제주시청(광양방면)의 위도 경도\n",
    "coords_jejuairport = (33.50661, 126.49345) #제주국제공항(구제주방면)의 위도 경도\n",
    "coords_hallahosp = (33.48963, 126.486) #한라병원의 위도 경도\n",
    "coords_rotary = (33.49143, 126.49678) # 제주도청신제주로터리의 위도 경도\n",
    "coords_jejucenterhigh = (33.48902, 126.5392) #제주중앙여자고등학교의 위도 경도\n",
    "coords_jejumarket = (33.51315, 126.52706) #동문시장의 위도 경도\n",
    "coords_jejusclass = (33.47626, 126.48141) #제주고등학교/중흥S클래스의 위도 경도\n",
    "coords_centerroad = (33.51073, 126.5239) #중앙로(국민은행)의 위도 경도\n",
    "coords_fiveway = (33.48667, 126.48092) # 노형오거리의 위도 경도\n",
    "coords_law = (33.49363, 126.53476) # 제주지방법원(광양방면)의 위도 경도\n",
    "\n",
    "data['dis_jejusicheong1'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusicheong1).km for i in range(len(data))]\n",
    "data['dis_jejuairport'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejuairport).km for i in range(len(data))]\n",
    "data['dis_hallahosp'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_hallahosp).km for i in range(len(data))]\n",
    "data['dis_rotary'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_rotary).km for i in range(len(data))]\n",
    "data['dis_jejucenterhigh'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejucenterhigh).km for i in range(len(data))]\n",
    "data['dis_jejumarket'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejumarket).km for i in range(len(data))]\n",
    "data['dis_jejusclass'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusclass).km for i in range(len(data))]\n",
    "data['dis_centerroad'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_centerroad).km for i in range(len(data))]\n",
    "data['dis_fiveway'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_fiveway).km for i in range(len(data))]\n",
    "data['dis_law'] = [geopy.distance.geodesic((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_law).km for i in range(len(data))]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 126)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ride_sum'] = data['6~7_ride'] + data['7~8_ride'] + data['8~9_ride'] + data['9~10_ride'] + data['10~11_ride'] + data['11~12_ride'] \n",
    "data['takeoff_sum'] = data['6~7_takeoff'] + data['7~8_takeoff'] + data['8~9_takeoff'] + data['9~10_takeoff'] + data['10~11_takeoff'] + data['11~12_takeoff'] \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 131)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = data.groupby('date')['6~7_ride'].agg([('6~7_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['7~8_ride'].agg([('7~8_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['8~9_ride'].agg([('8~9_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['9~10_ride'].agg([('9~10_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['10~11_ride'].agg([('10~11_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    if x ==5:\n",
    "        return 1\n",
    "    elif x==6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 132)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['weekend'] = data['weekday'].apply(h)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    if x in ['2019-09-12','2019-09-13','2019-09-14','2019-10-03','2019-10-09']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 133)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['holiday'] = data['date'].apply(g) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_mean() :\n",
    "\n",
    "    df = data.reset_index(drop=True)\n",
    "    df.groupby('weekday')['18~20_ride'].mean()\n",
    "    df['weekdaymean']= 1\n",
    "\n",
    "    index0 = df.query('weekday==0').index\n",
    "    index1 = df.query('weekday==1').index\n",
    "    index2 = df.query('weekday==2').index\n",
    "    index3 = df.query('weekday==3').index\n",
    "    index4 = df.query('weekday==4').index\n",
    "    index5 = df.query('weekday==5').index\n",
    "    index6 = df.query('weekday==6').index\n",
    "\n",
    "    df.iloc[index0,-1] = 1.343710\n",
    "    df.iloc[index1,-1] = 1.375319\n",
    "    df.iloc[index2,-1] = 1.430856\n",
    "    df.iloc[index3,-1] = 1.256710\n",
    "    df.iloc[index4,-1] = 1.067439\n",
    "    df.iloc[index5,-1] = 1.062123\n",
    "    df.iloc[index6,-1] = 1.034282\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 134)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = week_mean()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 135)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['in_out_mean'] = 1\n",
    "inindex = data.query('in_out == \"시내\"').index\n",
    "outindex = data.query('in_out == \"시외\"').index\n",
    "\n",
    "data.iloc[inindex,-1] = 1.228499\n",
    "data.iloc[outindex,-1] = 2.044345\n",
    "data['congestion'] = data['congestion'].astype('int64')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_people() :\n",
    "    bts['bus_route_id'] = bts['bus_route_id'].astype(str)\n",
    "\n",
    "    f = bts.groupby(['bus_route_id','user_category'])['user_count'].agg([('승객수', 'sum')]).reset_index()\n",
    "\n",
    "    g = pd.pivot_table(f, values='승객수', index='bus_route_id', columns='user_category',fill_value=0).reset_index()\n",
    "    g.columns = ['bus_route_id', 'adult','kids','teen','elder','jang','jang2','ugong','ugong2']\n",
    "    g = g[['bus_route_id', 'adult','kids','teen','elder']]\n",
    "\n",
    "    # merge\n",
    "    df = pd.merge(data, g, how='left', on='bus_route_id')\n",
    "\n",
    "    # na preprocessing -&gt; mean value\n",
    "    df['adult'] = df['adult'].fillna(2363.077778)\n",
    "    df['kids'] = df['kids'].fillna(60.426984)\n",
    "    df['teen'] = df['teen'].fillna(448.277778)\n",
    "    df['elder'] = df['elder'].fillna(751.309524)\n",
    "                 \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bus_route_id'] = data['bus_route_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 139)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = category_people()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_people_ratio() :\n",
    "    \n",
    "    a = bts.groupby('bus_route_id')['user_count'].agg([('전체', 'sum')]).reset_index()\n",
    "    b = bts.groupby(['bus_route_id','user_category'])['user_count'].agg([('승객수', 'sum')]).reset_index()\n",
    "\n",
    "    c = pd.merge(b, a, on='bus_route_id')\n",
    "    c['비율'] = c['승객수']/c['전체']\n",
    "    c = pd.pivot_table(c, values='비율', index='bus_route_id', columns='user_category',fill_value=0).reset_index()\n",
    "    c.columns = ['bus_route_id', 'adult_prop','kids_prop','teen_prop','elder_prop','jang_prop','jang2_prop','ugong_prop','ugong2_prop']\n",
    "    f = c[['bus_route_id', 'adult_prop','kids_prop','teen_prop','elder_prop']]\n",
    "\n",
    "    df = pd.merge(data, f, how='left', on='bus_route_id')\n",
    "\n",
    "    # na preprocessing -&gt; mean value\n",
    "    df['adult_prop'] = df['adult_prop'].fillna(0.549702)\n",
    "    df['kids_prop'] = df['kids_prop'].fillna(60.426984)\n",
    "    df['teen_prop'] = df['teen_prop'].fillna(0.019902)\n",
    "    df['elder_prop'] = df['elder_prop'].fillna(0.235848)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643593, 143)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = category_people_ratio()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jeoju_love() :\n",
    "    \n",
    "    loc_data = pd.read_csv(\"data_address.csv\", encoding='cp949')\n",
    "    loc_life = pd.read_csv(\"life_address.csv\", encoding='cp949')\n",
    "    \n",
    "    loc_data = loc_data[['location','dong', 'si']]\n",
    "    loc_life = loc_life[['location','dong', 'si']]\n",
    "\n",
    "    df = pd.merge(data, loc_data, how='left', on='location')\n",
    "    \n",
    "    jeju_life = pd.read_csv(\"jeju_financial_life_data.csv\")\n",
    "    jeju_life['location'] = jeju_life['x_axis'].astype(str).str[:10] + ',' + jeju_life['y_axis'].astype(str).str[:10]\n",
    "    jeju_life2 = pd.merge(jeju_life, loc_life, how='left', on='location')\n",
    "\n",
    "    dong_f1 = jeju_life2.groupby(['dong'])[['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend']].mean().reset_index()\n",
    "    dong_f1.columns=['dong','mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self','mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend']\n",
    "\n",
    "    dong_f2 = jeju_life2.groupby(['dong'])[['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend']].sum().reset_index()\n",
    "    dong_f2.columns=['dong','sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self','sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income', 'sum_avg_spend']\n",
    "\n",
    "    dong_f3 = (jeju_life2.groupby(['dong'])['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend'].sum()/jeju_life2.groupby(['dong'])['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend'].sum().sum()).reset_index()\n",
    "    dong_f3.columns = ['dong','rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self','rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income', 'rate_avg_spend']\n",
    "\n",
    "    m_1 = pd.merge(dong_f1, dong_f2, how='left', on='dong')\n",
    "    m_2 = pd.merge(m_1, dong_f3, how='left', on='dong')\n",
    "    df = pd.merge(df, m_2, how='left', on='dong')\n",
    "\n",
    "    #  # na preprocessing -&gt; mean value\n",
    "    df['mean_job_majorc'] = df['mean_job_majorc'].fillna(0.024219)\n",
    "    df['mean_job_smallc'] = df['mean_job_smallc'].fillna(0.145757)\n",
    "    df['mean_job_public'] = df['mean_job_public'].fillna(0.032768)\n",
    "    df['mean_job_profession'] = df['mean_job_profession'].fillna(0.014855)\n",
    "    df['mean_job_self'] = df['mean_job_self'].fillna(0.222090)\n",
    "    df['mean_vehicle_own_rat'] = df['mean_vehicle_own_rat'].fillna(0.041161)\n",
    "    df['mean_avg_income'] = df['mean_avg_income'].fillna(34221420)\n",
    "    df['mean_med_income'] = df['mean_med_income'].fillna(30645290)\n",
    "    df['mean_avg_spend'] = df['mean_avg_spend'].fillna(4224923)\n",
    "\n",
    "    df['sum_job_majorc'] = df['sum_job_majorc'].fillna(3.717861e+00)\n",
    "    df['sum_job_smallc'] = df['sum_job_smallc'].fillna(2.078142e+01)\n",
    "    df['sum_job_public'] = df['sum_job_public'].fillna(4.747755e+00)\n",
    "    df['sum_job_profession'] = df['sum_job_profession'].fillna(2.169554e+00)\n",
    "    df['sum_job_self'] = df['sum_job_self'].fillna(3.044199e+01)\n",
    "    df['sum_vehicle_own_rat'] = df['sum_vehicle_own_rat'].fillna(5.609080e+00)\n",
    "    df['sum_avg_income'] = df['sum_avg_income'].fillna(4.998226e+09)\n",
    "    df['sum_med_income'] = df['sum_med_income'].fillna(4.455924e+09)\n",
    "    df['sum_avg_spend'] = df['sum_avg_spend'].fillna(6.147678e+08)\n",
    "\n",
    "    df['rate_job_majorc'] = df['rate_job_majorc'].fillna(1.388889e-02)\n",
    "    df['rate_job_smallc'] = df['rate_job_smallc'].fillna(1.388889e-02)\n",
    "    df['rate_job_public'] = df['rate_job_public'].fillna(1.388889e-02)\n",
    "    df['rate_job_profession'] = df['rate_job_profession'].fillna(1.388889e-02)\n",
    "    df['rate_job_self'] = df['rate_job_self'].fillna(1.388889e-02)\n",
    "    df['rate_vehicle_own_rat'] = df['rate_vehicle_own_rat'].fillna(1.388889e-02)\n",
    "    df['rate_avg_income'] = df['rate_avg_income'].fillna(1.388889e-02)\n",
    "    df['rate_med_income'] = df['rate_med_income'].fillna(1.388889e-02)\n",
    "    df['rate_avg_spend'] = df['rate_avg_spend'].fillna(1.388889e-02)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_address.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-98b5a3db665b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjeoju_love\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-804e99d413bd>\u001b[0m in \u001b[0;36mjeoju_love\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mjeoju_love\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mloc_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_address.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mloc_life\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"life_address.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_address.csv'"
     ]
    }
   ],
   "source": [
    "data = jeoju_love()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data[data['station_name'].str.contains('고등학교')]\n",
    "highschool = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('대학교')]\n",
    "university = list(g['station_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x in highschool:\n",
    "        return 1\n",
    "    elif x in university:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['school'] = data['station_name'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data[data['station_name'].str.contains('환승')]\n",
    "transfer = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('공항')]\n",
    "airport = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('터미널')]\n",
    "terminal = list(g['station_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x in transfer:\n",
    "        return 1\n",
    "    elif x in airport:\n",
    "        return 1\n",
    "    elif x in terminal:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transfer'] = data['station_name'].apply(f) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "df_encode = data[['dong']]\n",
    "df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "\n",
    "data['dong2']=df_encoded['dong']\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist() :\n",
    "    jeju=(33.51411, 126.52969) # 제주 측정소 근처\n",
    "    gosan=(33.29382, 126.16283) #고산 측정소 근처\n",
    "    seongsan=(33.38677, 126.8802) #성산 측정소 근처\n",
    "    po=(33.24616, 126.5653) #서귀포 측정소 근처\n",
    "\n",
    "    t1 = [geopy.distance.vincenty( (i,j), jeju).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t2 = [geopy.distance.vincenty( (i,j), gosan).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t3 = [geopy.distance.vincenty( (i,j), seongsan).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t4 = [geopy.distance.vincenty( (i,j), po).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "\n",
    "    data['dis_jeju'] = t1\n",
    "    data['dis_gosan']=t2\n",
    "    data['dis_seongsan']=t3\n",
    "    data['dis_po']=t4\n",
    "\n",
    "    total = pd.DataFrame(list(zip( t1,t2,t3,t4)),columns=['jeju','gosan','seongsan','po'] )\n",
    "    data['dist_name'] = total.apply(lambda x: x.argmin(), axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dist()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain3 = pd.read_csv(\"rain3.csv\")\n",
    "\n",
    "# train, test의 변수명과 통일시키고, NaN의 값은 0.0000으로 변경\n",
    "rain3 = rain3.rename(columns={\"일시\":\"date\",\"지점\":\"dist_name\"})\n",
    "rain3 = rain3.fillna(0.00000)\n",
    "rain3['date'] = pd.to_datetime(rain3['date'])\n",
    "\n",
    "rain3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, rain3, how='left',on=['dist_name','date'])\n",
    "data = pd.get_dummies(data,columns=['dist_name'])\n",
    "data = pd.get_dummies(data,columns=['si'])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rainy_day'] = data['강수량(mm)'].apply(f)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['69a'] = data['6~7_ride']+data['7~8_ride']+data['8~9_ride']\n",
    "data['912a']=data['9~10_ride']+data['10~11_ride']+data['11~12_ride']\n",
    "\n",
    "data['69b'] = data['6~7_takeoff']+data['7~8_takeoff']+data['8~9_takeoff']\n",
    "data['912b'] = data['9~10_takeoff']+data['10~11_takeoff']+data['11~12_takeoff']\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.query('cue==\"0\"').reset_index()\n",
    "test_data = data.query('cue==\"1\"').reset_index()\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[['18~20_ride']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('X_train.csv', index = False, encoding = 'utf-8')\n",
    "test_data.to_csv('X_test.csv', index = False, encoding = 'utf-8')\n",
    "y_train.to_csv('y_train.csv', index = False, encoding = 'utf-8')\n",
    "\n",
    "print('save ..!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('X_train.csv', encoding = 'utf-8')\n",
    "test_data = pd.read_csv('X_test.csv', encoding = 'utf-8')\n",
    "y_train = pd.read_csv('y_train.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_0=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_1=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_2=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_3 =['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', \n",
    "           'elder_prop', 'mean_job_majorc', 'mean_job_smallc',\n",
    "           'mean_job_public', 'mean_job_profession', 'mean_job_self','mean_vehicle_own_rat',\n",
    "          '68a', '810a', '1012a', '68b', '810b', '1012b',\n",
    "          'dis_jeju', 'dis_gosan','dis_seongsan', 'dis_po','기온(°C)', '강수량(mm)', \n",
    "           'dist_name_gosan', 'dist_name_jeju','dist_name_po', 'dist_name_seongsan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_4 = ['sum_avg_spend', '68a', '810a', 'si_제주시',  'dong2' , 'bus_interval', 'dis_jejuairport', 'ride_sum',\n",
    "             'takeoff_sum', '1820_rs_mean', '1820_rs_sum','1820_r_mean','1820_r_sum', '1820_s_mean',\n",
    "             '1820_s_sum','congestion', 'bus_route_id2', '6~7_all_ride_number', '7~8_all_ride_number',\n",
    "             '8~9_all_ride_number', '1012a_mean', '1012b_sum','10~11_ride_sum', '10~11_takeoff_sum', '11~12_ride_sum',\n",
    "             '11~12_takeoff_sum', '1820_r_mean_sum', '1820_r_mean_mean',\n",
    "             '1820_r_sum_sum', '1820_r_sum_mean', '1820_rs_mean_sum',\n",
    "             '1820_s_mean_sum', '1820_s_mean_mean', '1820_s_sum_sum',\n",
    "             '1820_s_sum_mean', '1820_w_mean_sum', '1820_w_mean_mean',\n",
    "             '1820_w_sum_mean', '68a_sum', '68a_mean', '68b_sum', 'in_out', 'latitude', 'longitude',\n",
    "             '6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride', '6~7_takeoff', '7~8_takeoff',\n",
    "             '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "             'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "             'weekday_5', 'weekday_6', 'dis_jejusi', '68b_mean', '6~7_ride_sum',\n",
    "             '6~7_ride_mean', '6~7_takeoff_sum', '6~7_takeoff_mean', '7~8_ride_sum',\n",
    "             '7~8_ride_mean', '7~8_takeoff_sum', '7~8_takeoff_mean', '810a_sum',\n",
    "             '810b_sum', '8~9_ride_sum', '8~9_takeoff_sum', '8~9_takeoff_mean',\n",
    "             '9~10_ride_sum', '9~10_takeoff_sum', 'route_station_weekday2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMRegressor(num_iterations = 1000, \n",
    "                                learning_rate = 0.05,\n",
    "                                boosting = 'dart',\n",
    "                         Metric = 'regression_l2', n_jobs=-1)\n",
    "\n",
    "X = train_data[input_var_0]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns=range(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(test_data[input_var_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('lgbm0=229.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_features=5,\n",
    "                           min_samples_leaf=1,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=300,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_1]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf1=234.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=1217,\n",
    "                           max_features= 3,\n",
    "                           min_samples_leaf= 2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500)\n",
    "\n",
    "X = train_data[input_var_2]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf2=238.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_features=3,\n",
    "                           min_samples_leaf=2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_3]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf3=236.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_features=3,\n",
    "                           min_samples_leaf=2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_4]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf4=231.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "file_list = ['lgbm0=229.csv', 'rf1=234.csv', 'rf2=238.csv', 'rf3=236.csv', 'rf4=231.csv']\n",
    "\n",
    "for item in file_list :\n",
    "    if item.find('.csv') is not -1 :        \n",
    "        items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(items)) :\n",
    "    \n",
    "    item = items[i]\n",
    "    df = pd.read_csv(item, engine = 'python').iloc[:,1:]\n",
    "    df.columns = [items[i]]\n",
    "    \n",
    "    if i == 0 :\n",
    "        corr_df = pd.DataFrame()\n",
    "        \n",
    "    corr_df = pd.concat([corr_df, df], axis = 1)\n",
    "\n",
    "    \n",
    "corr = np.array(corr_df.corr().mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = []\n",
    "\n",
    "for i in range(0, len(items)) :\n",
    "    score = items[i].split('=')[-1][:6]\n",
    "    score = score.split('.')[0]\n",
    "    score = float(score)\n",
    "    rmse.append(score)\n",
    "    \n",
    "df = pd.DataFrame({'rnk': list(range(0, len(rmse))), 'rmse': rmse, 'cor': corr})\n",
    "df.index = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('cor', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=30)\n",
    "\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.00005 , df.rmse[line]-0.00003, \n",
    "            df.rnk[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.00002,df.cor.max()+0.0002))\n",
    "plt.ylim((df.rmse.min()-0.0002,df.rmse.max()+0.0002))\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
